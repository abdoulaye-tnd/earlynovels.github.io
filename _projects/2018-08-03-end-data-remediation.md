---
layout: project
title: END Data Remediation
author: samherron
tags: [projects]
img: data-remed-2.png
description: >
---

![Screenshot of END cataloging guide and a MARC XML record.](/public/img/data-remed-2.png)


The Early Novels Database is nearly a decade old, and so we have nearly ten summers worth of records. This is exciting! But as the data amasses, some of its shortcomings come into focus: Over the course of ten years, cataloging protocols have changed, spellings errors have piled up, documentation has disappeared, new catalogers each summer have made different decisions about how things are to be done… Looking through END’s data on OpenRefine reveals datafields no longer used, a million ways to spell “London”, records without publishing dates or cataloging dates, records with multiple narrative form fields (there should only be one per record), blank records which, for reasons we can’t understand, only have END’s signature in the 710 field and no other doctoring from the original Franklin record, etc. The list goes on.

I spent this summer doing just a small piece of the necessary data remediation: control terms. Many of END’s datafields require the use of certain controlled terms (as opposed to transcription fields or note fields) that allow researchers to find the corpus of works they want or need. For example, in the 592$a subfield--the primary narrative form--the only options are “Epistolary”, “First-person”, “Third-person”, or “Dramatic dialogue”. In these fields, punctuation, spelling, and capitalization is crucial, otherwise those mistyped records are more difficult to find. Researchers look for “First-person” and so rightfully miss “First Person” “First person” “Firstperson” “First-Person”, variations that not been corrected during first or second review, and haven’t been touched since their original validation, and so peppered our data at the beginning of the summer.

Treating the records as one giant text document, I used MarcEdit to find and replace mistyped terms with their correct control terms. This was mostly tedious, and there was a huge range of impact for each find/replace move. For example, in the 599$b subfield, where we describe the type of author claim we find (Proper name, Reference to other works, Generic/Descriptive), changing “Generic/descriptive” to “Generic/Descriptive” impacted 432 instances. Other times, fixing individual spelling mistakes (say, “Genertic/Descriptive” to “Generic/Descriptive” in the same field), I’d only be impacting one record at a time. Luckily, find and replace functions are relatively speedy, which made relatively quick work of it. All in all, I made over 200 find and replace switches, impacting thousands of control terms, hopefully making the data easier to navigate, especially in its tabular formats through OpenRefine and Excel.


![Screenshot featuring END data in OpenRefine and MarcEdit.](/public/img/data-remed-1.png)


Figuring out what I would do and how I would do it was far more complicated than making the changes I eventually made. A lot of our data is great, but not short on problems, each of which present unique issues. I wanted to make the most impact in the time that I had, and I believe I chose wisely, but there are a lot of issues I couldn’t begin to broach:

For example, changes in cataloging protocol presented some interesting discrepancies. We used to use the 920 data field to record an enhanced title field for paratexutal titles--i.e. noting which words in a Dedication title was a nouns, adjectives, names, etc. This is only a guess, based on what the handful of extant 920 fields contain given that we don’t have any documentation of early cataloging protocols. From the position of data remediation, I’m presented with some questions: Do we delete them entirely? Keep them as legacy metadata? Just add a note field to all of them marking them as legacy? Or another problem: The narrative form field, 592, shouldn’t have a volume subfield, but for anthologized works, individual catalogers have used a $v subfield to show that different volumes contain different works in different narrative forms. I’d like to delete these subfields for the reason of standardization, but the $v subfield has important information there that I don’t necessarily want to trash across the board. What then?

Some of the questions are very technical: END doesn’t have an exacting protocol for how to capture footnotes. Do we make a new 520 field for each volume? Should we make two per volume if there are footnotes to its paratexts as well as footnotes in the main text? Just one 520 field with multiple volume and location subfields? There are a few reasonable answers to this question, but because of differences in personal cataloging styles, it’s hard to say what calls were made just looking at the data, nevermind how to solve them and standardize them. These kinds of issues exist for nearly every datafield, and presented challenges simply to imagining how I might pursue remediation.

Other problems are less systematic: Typos. Typos are fairly difficult to find, though a spell check function might help you out a bit. Even so, it would be hard to say whether the typo is the fault of the cataloger or if it’s native to the text, unless you had the book in front of you.

Finally, some problems are effectively unsolvable: If a cataloger accidentally missed an interesting piece of marginalia, failed to catalog a half title page, forgot to put something in the 500 field, etc., those things would be impossible to identify as missing. The records would be incomplete in ways that we aren’t able to identify. “Remediating” such errors would require rigorous double-checking for discrepancies we aren’t even sure exist (though they probably do).

Exploring these problems was an exciting way to get to know the data. As a cataloger and researcher, I usually only have the opportunity to see the input data (individual records), and certain tabular representations of the compiled output (in Excel or OpenRefine). Remediation gave me the opportunity to search somewhat aimlessly through the data, identifying problems, looking in places and datafields I rarely use, browsing the records chronologically by cataloging date...The data and its contours are often silly, narratively confusing, exciting, variously in-depth or bare-bones. Fixing control terms is obviously only one piece of the complicated puzzle that is END records. Further remediation will be necessary to fix the variety of other problems I couldn’t get to this summer, couldn’t figure out how to solve, couldn’t locate, didn’t think of as problems, and so on. But until then, data users will have an easier time finding what they need to find in our data...I hope!
